{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zUEK1KNlA28O"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "import numpy as np\n",
    "from  utils.dataset import Dataset\n",
    "# dataset = Dataset(image_size=(64,64),data_base_path=\"./data\",batch_size=32)\n",
    "# train_ds = dataset.get_train_ds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "sO375URWPcZC"
   },
   "outputs": [],
   "source": [
    "from re import template\n",
    "from numpy.lib.type_check import imag\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as  K \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers as L\n",
    "import time\n",
    "import os\n",
    "from models.stackgan import Stage1Model \n",
    "model = Stage1Model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ADvYKlTxv3P7"
   },
   "outputs": [],
   "source": [
    "model.load_weights(path=\"./weights/weights_50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qHfBb1fPA28Y",
    "outputId": "ff420bb4-e672-4c7c-85ac-a1e6161d5ffd"
   },
   "outputs": [],
   "source": [
    "model.train(train_ds,batch_size=32,num_epochs=100,save_weights_epoch=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "pI9uhvhlWFoZ"
   },
   "outputs": [],
   "source": [
    "dataset = Dataset(image_size=(256,256),data_base_path=\"./data\",batch_size=2)\n",
    "train_ds = dataset.get_test_ds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "neAbJUlJNiV9",
    "outputId": "cb484bf9-571d-49c3-892a-7fc540ddbc52"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2933"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.test_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "0I3i7kPFKM8I"
   },
   "outputs": [],
   "source": [
    "def ResidualBlock(input, num_filters):\n",
    "    x  =  L.Conv2D(filters = num_filters, kernel_size= 3 , strides=1, padding='same')(input)\n",
    "    x  =  L.BatchNormalization()(x)\n",
    "    x  =  L.ReLU()(x)\n",
    "    x  =  L.Conv2D(filters = num_filters, kernel_size=3, strides=1, padding='same')(x)\n",
    "    x  =  L.BatchNormalization()(x)\n",
    "    x  =  L.ReLU()(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "class Stage2Generator(keras.Model):\n",
    "    def __init__(self,*args, **kwargs):\n",
    "        super(Stage2Generator, self).__init__(*args, **kwargs)\n",
    "        self.augmentation = ConditioningAugmentation()\n",
    "        self.reshape = tf.keras.layers.Reshape(target_shape = (1, 1, 128))\n",
    "\n",
    "    def call(self,inputs):\n",
    "        image , embedding = inputs\n",
    "        c,phi = self.augmentation(embedding)\n",
    "        c = K.expand_dims(c, axis=1)\n",
    "        c = K.expand_dims(c, axis=1)\n",
    "        c = K.tile(c, [1, 16, 16, 1])\n",
    "        x = DownSamplingBlock(image,num_filters=64,kernel_size = 3, strides = 1,batch_norm=False)\n",
    "        x = DownSamplingBlock(x,num_filters=256)\n",
    "        x = DownSamplingBlock(x,num_filters=512)\n",
    "        x = K.concatenate([c, x], axis = 3)        \n",
    "        x = ResidualBlock(x, 128)\n",
    "        x = ResidualBlock(x, 256)\n",
    "        x = ResidualBlock(x, 128)\n",
    "        x = UpSamplingBlock(x,256)\n",
    "        x = ResidualBlock(x,256)\n",
    "        x = UpSamplingBlock(x,256)\n",
    "        x = ResidualBlock(x,128)\n",
    "        x = UpSamplingBlock(x,256)\n",
    "        x = ResidualBlock(x,128)\n",
    "        x = UpSamplingBlock(x,3)\n",
    "        \n",
    "        return x,phi\n",
    "\n",
    "def DownSamplingBlock(  inputs,\n",
    "                        num_filters, \n",
    "                        kernel_size= 4,\n",
    "                        strides = 2,\n",
    "                        batch_norm=True,\n",
    "                        activation= True):\n",
    "    x = L.Conv2D(filters = num_filters, kernel_size= kernel_size , strides=strides, padding='same')(inputs)\n",
    "    if batch_norm:\n",
    "        x = L.BatchNormalization()(x)\n",
    "    if activation:\n",
    "        x = L.LeakyReLU()(x)\n",
    "    #print(f\" x shape {x.shape}\")\n",
    "    return x\n",
    "\n",
    "\n",
    "class Stage2Discriminator(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super(Stage2Discriminator, self).__init__()\n",
    "    self.embed = EmbeddingCompresssor()\n",
    "    self.reshape = tf.keras.layers.Reshape(target_shape = (1, 1, 128))\n",
    "    self.conv_out = tf.keras.layers.Conv2D(filters = 1, kernel_size = 4, strides = 1, padding = \"valid\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n",
    "\n",
    "  def call(self, inputs):\n",
    "    I, E = inputs\n",
    "    T = self.embed(E)\n",
    "    T = self.reshape(T)\n",
    "    T = tf.tile(T, (1, 4, 4, 1))\n",
    "    \n",
    "    x = DownSamplingBlock(I,num_filters=64,batch_norm=False)\n",
    "    x = DownSamplingBlock(x,num_filters=128)\n",
    "    x = DownSamplingBlock(x, num_filters=256)\n",
    "    x = DownSamplingBlock(x,num_filters=512)\n",
    "    x = DownSamplingBlock(x,num_filters=1024)\n",
    "    x = DownSamplingBlock(x,num_filters=512)\n",
    "    x = DownSamplingBlock(x,num_filters=128,kernel_size=1,strides=1)\n",
    "    \n",
    "    y = DownSamplingBlock(x, num_filters=128,kernel_size=1,strides=1)\n",
    "    y = DownSamplingBlock(y, num_filters=256,kernel_size=3,strides=1)\n",
    "    y = DownSamplingBlock(y, num_filters=128,kernel_size=3,strides=1)\n",
    "\n",
    "    A = tf.keras.layers.Add()([x,y])\n",
    "    A = tf.nn.leaky_relu(A)\n",
    "    merged_input = tf.keras.layers.concatenate([A, T])\n",
    "    \n",
    "    z = DownSamplingBlock(merged_input,128,kernel_size=1,strides=1)\n",
    "    z = self.conv_out(z)\n",
    "    z = tf.squeeze(z)\n",
    "    return z\n",
    "\n",
    "\n",
    "class Stage2Model(keras.Model):\n",
    "    def __init__(self,stage1model):\n",
    "        super(Stage2Model, self).__init__()\n",
    "        self.generator1 = stage1model.generator\n",
    "        self.generator2 = Stage2Generator()\n",
    "        self.discriminator2 = Stage2Discriminator()\n",
    "        self.generator2_optimizer = keras.optimizers.Adam(learning_rate= 0.0001, beta_1= 0.5 , beta_2= 0.999)\n",
    "        self.discriminator2_optimizer = keras.optimizers.Adam(learning_rate= 0.0001, beta_1= 0.5 , beta_2= 0.999)\n",
    "        self.noise_dim = 100\n",
    "    def train(self, train_ds, batch_size= 64, num_epochs =1,steps_per_epoch =125):\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(\"Epoch %d/%d:\\n  \"%(epoch + 1, num_epochs), end = \"\")\n",
    "            start_time = time.time()\n",
    "            if epoch % 100 == 0:\n",
    "                K.set_value(self.generator2_optimizer.learning_rate, self.generator2_optimizer.learning_rate / 2)\n",
    "                K.set_value(self.discriminator2_optimizer.learning_rate, self.discriminator2_optimizer.learning_rate / 2)\n",
    "            \n",
    "            generator_loss_log = []\n",
    "            discriminator_loss_log = []\n",
    "            steps_per_epoch = steps_per_epoch\n",
    "            batch_iter = iter(train_ds) \n",
    "            for i in range(steps_per_epoch):\n",
    "                if i% 5 ==0:\n",
    "                    print(\"=\", end = \"\")\n",
    "                hr_image_batch,embedding_batch = next(batch_iter)\n",
    "                batch_size = hr_image_batch.shape[0]\n",
    "                z_noise = tf.random.normal((batch_size, self.noise_dim))\n",
    "                mismatched_images = tf.roll(hr_image_batch, shift=1, axis = 0)\n",
    "                real_labels = tf.random.uniform(shape = (batch_size, ), minval = 0.9, maxval=1.0)\n",
    "                fake_labels = tf.random.uniform(shape = (batch_size, ), minval = 0.0 , maxval = 0.1)\n",
    "                mismatched_labels = tf.random.uniform(shape = (batch_size, ), minval = 0.0, maxval = 0.1)\n",
    "                with tf.GradientTape() as generator_tape, tf.GradientTape() as discriminator_tape:\n",
    "                    lr_fake_images , _ = self.generator1([embedding_batch,z_noise])\n",
    "                    hr_fake_images  ,  phi  = self.generator2([lr_fake_images,embedding_batch])\n",
    "                    real_logits = self.discriminator2([hr_image_batch, embedding_batch])\n",
    "                    del hr_image_batch  ## clear memory used by hr_image_batch \n",
    "                    fake_logits = self.discriminator2([hr_fake_images, embedding_batch])\n",
    "                    del hr_fake_images  ## clear memory used by hr_fake_images \n",
    "                    mismatched_logits = self.discriminator2([mismatched_images, embedding_batch])\n",
    "                    del mismatched_images  ## clear memory used by mismatched_images\n",
    "                    l_sup = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(real_labels,real_logits))\n",
    "                    l_klreg = KL_loss(tf.random.normal((phi.shape[0], phi.shape[1])), phi)\n",
    "                    generator_loss = l_sup + 2.0*l_klreg\n",
    "                    \n",
    "                    \n",
    "                    l_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(real_labels,real_logits))\n",
    "                    l_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(fake_logits,fake_labels))\n",
    "                    l_mismatched = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(mismatched_logits,mismatched_labels))\n",
    "                    discriminator_loss = 0.5*tf.add(l_real,tf.add(l_fake, l_mismatched))\n",
    "\n",
    "                generator_gradients = generator_tape.gradient(generator_loss,self.generator2.trainable_variables)\n",
    "                self.generator2_optimizer.apply_gradients(zip(generator_gradients, self.generator2.trainable_variables))             \n",
    "                del generator_gradients\n",
    "                discriminator_gradients = discriminator_tape.gradient(discriminator_loss,self.discriminator2.trainable_variables)\n",
    "                self.discriminator2_optimizer.apply_gradients(zip(discriminator_gradients, self.discriminator2.trainable_variables))\n",
    "                del discriminator_gradients\n",
    "                generator_loss_log.append(generator_loss)\n",
    "                discriminator_loss_log.append(discriminator_loss)\n",
    "\n",
    "                end_time = time.time()\n",
    "\n",
    "            if epoch % 1 == 0:\n",
    "                epoch_time = end_time - start_time\n",
    "                template = \" - generator_loss: {:.4f} - discriminator_loss: {:.4f} - epoch_time: {:.2f} s\"\n",
    "                print(template.format(tf.reduce_mean(generator_loss_log), tf.reduce_mean(discriminator_loss_log), epoch_time))\n",
    "\n",
    "            if (epoch + 1) % 10 == 0 or epoch == num_epochs - 1:\n",
    "                save_path = \"./hr_results/epoch_\" + str(epoch + 1)\n",
    "                temp_embeddings = None\n",
    "                for _, embeddings in train_ds:\n",
    "                    temp_embeddings = embeddings.numpy()\n",
    "                    break\n",
    "                if os.path.exists(save_path) == False:\n",
    "                    os.makedirs(save_path)\n",
    "                temp_batch_size = temp_embeddings.shape[0]\n",
    "                temp_z_noise = tf.random.normal((temp_batch_size, self.noise_dim))\n",
    "                temp_embedding_batch = temp_embeddings[0:temp_batch_size]\n",
    "                lr_temp_images,_ = self.generator1([temp_embedding_batch, temp_z_noise])\n",
    "                fake_images,_= self.generator2([lr_temp_images,temp_embedding_batch])\n",
    "                for i, image in enumerate(fake_images):\n",
    "                        image = 127.5*image + 127.5\n",
    "                        image = image.numpy().astype('uint8')\n",
    "                        image  = keras.preprocessing.image.array_to_img(image)\n",
    "                        image.save(save_path + \"/gen_%d.png\"%(i))\n",
    "\n",
    "                weights_path = f\"./weights/hr_weights_{epoch+1}\"\n",
    "                if os.path.exists(weights_path)== False:\n",
    "                    os.makedirs(weights_path)\n",
    "                self.generator2.save_weights(weights_path+\"/stage2_generator.h5\")\n",
    "                self.discriminator2.save_weights(weights_path+\"/stage2_discriminator.h5\")\n",
    "model2= Stage2Model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "id": "vMKBd1CxWsuc",
    "outputId": "2f2fdc56-c21c-44dd-9f77-e52be1e24943"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:\n",
      "  ="
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[2,128,256,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Conv2DBackpropInput]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-81022fdcebfb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-23-eb4d0f30123a>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_ds, batch_size, num_epochs, steps_per_epoch)\u001b[0m\n\u001b[0;32m    142\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerator2_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator_gradients\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerator2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m                 \u001b[1;32mdel\u001b[0m \u001b[0mgenerator_gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m                 \u001b[0mdiscriminator_gradients\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiscriminator_tape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiscriminator_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiscriminator2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiscriminator2_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiscriminator_gradients\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiscriminator2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m                 \u001b[1;32mdel\u001b[0m \u001b[0mdiscriminator_gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1071\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1072\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1073\u001b[1;33m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[0;32m   1074\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1075\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[0;32m    160\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\nn_grad.py\u001b[0m in \u001b[0;36m_Conv2DGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m    594\u001b[0m           \u001b[0mexplicit_paddings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m           \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 596\u001b[1;33m           data_format=data_format),\n\u001b[0m\u001b[0;32m    597\u001b[0m       gen_nn_ops.conv2d_backprop_filter(\n\u001b[0;32m    598\u001b[0m           \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d_backprop_input\u001b[1;34m(input_sizes, filter, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[0;32m   1253\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1254\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1255\u001b[1;33m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1256\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1257\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   6841\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\" name: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6842\u001b[0m   \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6843\u001b[1;33m   \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6844\u001b[0m   \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6845\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[2,128,256,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Conv2DBackpropInput]"
     ]
    }
   ],
   "source": [
    "model2.train(train_ds,batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8WEh-oXVOqiJ"
   },
   "outputs": [],
   "source": [
    "  "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "train.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "9f323a07508cb1307c6e6553fdda3bd6336c2794a5b8da8edb0e72318bc1a6d7"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
