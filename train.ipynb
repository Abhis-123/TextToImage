{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "import numpy as np\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  utils.dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(image_size=(64,64),data_base_path=\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = dataset.get_train_ds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as  K \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers as L\n",
    "import time\n",
    "############################################################\n",
    "# Conditioning Augmentation Network\n",
    "############################################################\n",
    "class ConditioningAugmentation(keras.Model):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(ConditioningAugmentation, self).__init__(*args, **kwargs)\n",
    "        self.dense = L.Dense(256)\n",
    "        self.activation = L.LeakyReLU(alpha=0.2)\n",
    "        \n",
    "\n",
    "    def call(self,input):\n",
    "        x  = self.dense(input)\n",
    "        phi= self.activation(x)\n",
    "        mean = phi[:,:128]\n",
    "        std =tf.math.exp(phi[:,128:])\n",
    "        epsilon = K.random_normal(shape = K.constant((mean.shape[1], ), dtype = 'int32'))\n",
    "        output = mean + std*epsilon\n",
    "        return output,phi\n",
    "\n",
    "class EmbeddingCompresssor(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(EmbeddingCompresssor, self).__init__()\n",
    "        self.dense = L.Dense(128)\n",
    "\n",
    "    def call(self,input):\n",
    "        x = self.dense(input)\n",
    "        x = L.LeakyReLU(0.2)(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "############################################################\n",
    "# Stage 1 Generator Network (CGAN)\n",
    "############################################################\n",
    "\n",
    "\n",
    "def UpSamplingBlock(input,num_filters):\n",
    "    x = L.UpSampling2D(size=2)(input)\n",
    "    x = L.Conv2D(num_filters,kernel_size=3,padding='same',strides=1,use_bias=False)(x)\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.ReLU()(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "class Stage1Generator(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Stage1Generator, self).__init__()\n",
    "        self.augmentation = ConditioningAugmentation()\n",
    "        self.concat = L.Concatenate(axis=1)\n",
    "        self.dense = tf.keras.layers.Dense(units = 128*8*4*4, kernel_initializer = tf.random_normal_initializer(stddev = 0.02))\n",
    "        self.reshape = tf.keras.layers.Reshape(target_shape = (4, 4, 128*8), input_shape = (128*8*4*4, ))\n",
    "        self.batchnorm1 = tf.keras.layers.BatchNormalization(axis = -1, momentum = 0.99)\n",
    "        self.activation = L.ReLU()\n",
    "\n",
    "    def call(self,inputs):\n",
    "        embedding , noise = inputs\n",
    "        c , phi = self.augmentation(embedding)\n",
    "        gen_input = self.concat([c,noise])\n",
    "        \n",
    "        x = self.dense(gen_input)\n",
    "        x = self.reshape(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.activation(x)\n",
    "        x = UpSamplingBlock(x, 512)\n",
    "        x = UpSamplingBlock(x, 256)\n",
    "        x = UpSamplingBlock(x,128)\n",
    "        x = UpSamplingBlock(x,3)\n",
    "        x = L.Conv2D(3,kernel_size=3,padding='same')(x)\n",
    "        x = L.Activation('tanh')(x)\n",
    "\n",
    "        return x,phi\n",
    "\n",
    "class Stage1Discriminator(keras.Model):\n",
    "    def __init__(self,*args, **kwargs):\n",
    "        super(Stage1Discriminator, self).__init__(*args,**kwargs)\n",
    "        \n",
    "        self.l1 = L.Conv2D(64,kernel_size=4,strides=2,padding='same',kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev = 0.02 ))\n",
    "        self.l2 = L.Conv2D(128,kernel_size=4,strides=2,padding='same',kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev =0.02))\n",
    "        self.l3 = L.BatchNormalization(axis = -1)\n",
    "        self.l4 = L.Conv2D(256,kernel_size=4,strides=2,padding='same',kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n",
    "        self.l5 = L.BatchNormalization(axis = -1)\n",
    "        self.l6 = L.Conv2D(516,kernel_size=4,strides=2,padding='same',kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n",
    "        self.l7 = L.BatchNormalization(axis = -1)\n",
    "        self.embedding = EmbeddingCompresssor()\n",
    "        self.l9 = L.Reshape(target_shape = (1,1,128))\n",
    "        self.concat= L.Concatenate() \n",
    "        self.l11= L.Conv2D(filters = 1024, kernel_size=4,strides=2,padding='same',kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n",
    "        self.l12= L.BatchNormalization(axis = -1)\n",
    "        self.l13= L.Conv2D(filters = 1, kernel_size=4,strides=2,padding='same',kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n",
    "        \n",
    "\n",
    "    def call(self,inputs):\n",
    "        I , E = inputs #\n",
    "        x  = self.l1(I)\n",
    "        x  = self.l2(x)\n",
    "        x  = self.l3(x)\n",
    "        x  = self.l4(x)\n",
    "        x  = self.l5(x)\n",
    "        x  = self.l6(x)\n",
    "        x  = self.l7(x)\n",
    "\n",
    "        t  = self.embedding(E)\n",
    "        t  = self.l9(t)\n",
    "        t  = tf.tile(t,(1,4,4,1))\n",
    "\n",
    "        merged_input  = self.concat([t,x])\n",
    "\n",
    "        y = self.l11(merged_input)\n",
    "        y =self.l12(y)\n",
    "        y =L.LeakyReLU()(y)\n",
    "\n",
    "        y = self.l13(y)\n",
    "\n",
    "        return tf.squeeze(y)\n",
    "\n",
    "\n",
    "def KL_loss(y_true, y_pred):\n",
    "  mean = y_pred[:, :128]\n",
    "  logsigma = y_pred[:, 128:]\n",
    "  loss = -logsigma + 0.5*(-1 + K.exp(2.0*logsigma) + K.square(mean))\n",
    "  loss = K.mean(loss)\n",
    "  return loss\n",
    "\n",
    "class Stage1Model(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Stage1Model, self).__init__()\n",
    "        self.generator = Stage1Generator() \n",
    "        self.discriminator= Stage1Discriminator()\n",
    "        self.generator_optimizer = keras.optimizers.Adam(0.001)\n",
    "        self.discriminator_optimizer = keras.optimizers.Adam(0.001)\n",
    "        self.noise_dim = 100 #\n",
    "        self.c_dim = 128 \n",
    "        self.loss = {}\n",
    "\n",
    "    def train(self,train_data,num_epochs=10,steps_per_epoch=125):\n",
    "        noise_dim = self.noise_dim\n",
    "        for epoch in range(num_epochs):\n",
    "            print(\"Epoch %d/%d:\\n |\"%(epoch+1,num_epochs),end=\"\")\n",
    "            start_time = time.time()\n",
    "            if epoch % 100 == 0:\n",
    "                K.set_value(self.generator_optimizer.learning_rate, self.generator_optimizer.learning_rate / 2)\n",
    "                K.set_value(self.discriminator_optimizer.learning_rate, self.discriminator_optimizer.learning_rate / 2)\n",
    "            \n",
    "            generator_loss_log = []\n",
    "            discriminator_loss_log =[]\n",
    "            steps_per_epoch = steps_per_epoch\n",
    "            # batch iterator Generator\n",
    "            batch_iter = iter(train_data)\n",
    "            steps_per_epoch_batch_len = steps_per_epoch//15\n",
    "\n",
    "            batch_size = next(batch_iter)[0].shape[0]\n",
    "            for i in range(steps_per_epoch):\n",
    "                if steps_per_epoch % steps_per_epoch_batch_len == 0:\n",
    "                    print(\"=\",end=\"\")\n",
    "                image_batch, embedding_batch = next(batch_iter)\n",
    "                noise = tf.random.normal(shape = (batch_size, noise_dim))\n",
    "\n",
    "                mismatched_images = tf.roll(image_batch ,shift = 1,axis =0 )\n",
    "                real_labels = tf.random.uniform(shape = (batch_size,),minval = 0.9 , maxval =1)\n",
    "                fake_labels = tf.random.uniform(shape = (batch_size, ), minval = 0.0, maxval = 0.1)\n",
    "                mismatched_labels = tf.random.uniform(shape = (batch_size,), minval=0.0 , maxval = 0.1)\n",
    "\n",
    "                with tf.GradientTape() as generator_tape, tf.GradientTape() as discriminator_tape:\n",
    "                    fake_images , phi = self.generator([embedding_batch,noise])\n",
    "                    real_logits = self.discriminator([image_batch,embedding_batch])\n",
    "                    fake_logits = self.discriminator([fake_images, embedding_batch])\n",
    "                    mismatched_logits = self.discriminator([mismatched_images, embedding_batch])\n",
    "                    \n",
    "                    l_sup = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(real_labels,real_logits))  \n",
    "                    l_klreg = KL_loss(tf.random.normal((phi.shape[0], phi.shape[1])), phi)\n",
    "                    generator_loss = l_sup + 2.0*l_klreg\n",
    "\n",
    "                    l_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(real_labels, real_logits))\n",
    "                    l_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(fake_labels, fake_logits))\n",
    "                    l_mismatched = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(mismatched_labels, mismatched_logits))\n",
    "                    discriminator_loss = 0.5*tf.add(l_real, 0.5*tf.add(l_fake, l_mismatched))\n",
    "                    \n",
    "                generator_gradients = generator_tape.gradient(generator_loss,self.generator.trainable_variables)\n",
    "                discriminator_gradients = discriminator_tape.gradient(discriminator_loss,self.discriminator.trainable_variables)\n",
    "                self.generator_optimizer.apply_gradients(zip(generator_gradients, self.generator.trainable_variables))\n",
    "                self.discriminator_optimizer.apply_gradients(zip(discriminator_gradients, self.discriminator.trainable_variables))\n",
    "                \n",
    "                generator_loss_log.append(generator_loss)\n",
    "                discriminator_loss_log.append(discriminator_loss)\n",
    "\n",
    "            end_time = time.time()\n",
    "\n",
    "            if epoch % 1 == 0:\n",
    "                epoch_time = end_time - start_time\n",
    "                template = \" - generator_loss: {:.4f} - discriminator_loss: {:.4f} - epoch_time: {:.2f} s\"\n",
    "                print(template.format(tf.reduce_mean(generator_loss_log), tf.reduce_mean(discriminator_loss_log), epoch_time))\n",
    "\n",
    "            if (epoch + 1) % 10 == 0 or epoch == num_epochs - 1:\n",
    "                import os\n",
    "                save_path = \"./lr_results/epoch_\" + str(epoch + 1)\n",
    "                temp_embeddings = None\n",
    "                for _, embeddings in train_data:\n",
    "                    temp_embeddings = embeddings.numpy()\n",
    "                    break\n",
    "                if os.path.exists(save_path) == False:\n",
    "                    os.makedirs(save_path)\n",
    "                \n",
    "                temp_batch_size = 10\n",
    "                temp_z_noise = tf.random.normal((temp_batch_size, self.noise_dim))\n",
    "                temp_embedding_batch = temp_embeddings[0:temp_batch_size]\n",
    "                fake_images, _ = self.generator([temp_embedding_batch, temp_z_noise])\n",
    "               \n",
    "                for i, image in enumerate(fake_images):\n",
    "                    image = 127.5*image + 127.5\n",
    "                    image = image.numpy().astype('uint8')\n",
    "                    image = keras.preprocessing.image.array_to_img(image)\n",
    "                    image.save(save_path + \"/gen_%d.png\"%(i))\n",
    "                    # cv2.imwrite(save_path + \"/gen_%d.png\"%(i), image)\n",
    "                \n",
    "                self.generator.save_weights(\"./weights/stage1_generator_\" + str(epoch + 1) + \".ckpt\")\n",
    "                self.discriminator.save_weights(\"./weights/stage1_discriminator_\" + str(epoch + 1) + \".ckpt\")\n",
    "                \n",
    "                \n",
    "model = Stage1Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:\n",
      " [=========================] - generator_loss: 1.3858 - discriminator_loss: 0.6030 - epoch_time: 165.05 s\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-dee2723daf17>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-19-0ed36a75edb4>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_ds, batch_size, num_epochs, z_dim, c_dim, stage1_generator_lr, stage1_discriminator_lr)\u001b[0m\n\u001b[0;32m    203\u001b[0m           \u001b[0mtemp_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membeddings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m           \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m           \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[0mtemp_batch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "model.train(train_ds,num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9f323a07508cb1307c6e6553fdda3bd6336c2794a5b8da8edb0e72318bc1a6d7"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
