{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "interpreter": {
      "hash": "9f323a07508cb1307c6e6553fdda3bd6336c2794a5b8da8edb0e72318bc1a6d7"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiLdd2uM6PE5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sx0dRRDC6cQ",
        "outputId": "a4326790-627e-4f9d-b6ee-0bd7649eef1c"
      },
      "source": [
        "%cd \"/content/drive/MyDrive/TextToImage\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1l8NtTazjhx8qLF-6HwW1IQ1voe-BgiJK/TextToImage\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUEK1KNlA28O"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras \n",
        "import numpy as np\n",
        "from  utils.dataset import Dataset\n",
        "from re import template\n",
        "from numpy.lib.type_check import imag\n",
        "from tensorflow.keras import backend as  K \n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers as L\n",
        "import time\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ym0Rb6Mhw6Nm"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver('grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "tf.config.experimental_connect_to_cluster(resolver)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1xGbfxtxA_d",
        "outputId": "c840e1eb-2143-407b-fd23-5404fef584d8"
      },
      "source": [
        "\n",
        "# This is the TPU initialization code that has to be at the beginning.\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "print(\"All devices: \", tf.config.list_logical_devices('TPU'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.34.86.250:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.34.86.250:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "All devices:  [LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:7', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:6', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:5', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:4', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:3', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:0', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:1', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:2', device_type='TPU')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xz62GbO0xIZm",
        "outputId": "7e08ff49-ad38-47d9-947a-220e8540e9c1"
      },
      "source": [
        "strategy = tf.distribute.experimental.TPUStrategy(resolver)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ryJLiouwzvX"
      },
      "source": [
        "dataset = Dataset(image_size=(64,64),data_base_path=\"./data\",batch_size=128)\n",
        "train_ds = dataset.get_train_ds()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sO375URWPcZC"
      },
      "source": [
        "\n",
        "############################################################\n",
        "# Conditioning Augmentation Network\n",
        "############################################################\n",
        "class ConditioningAugmentation(keras.Model):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(ConditioningAugmentation, self).__init__(*args, **kwargs)\n",
        "        self.dense = L.Dense(256)\n",
        "        self.activation = L.LeakyReLU(alpha=0.2)\n",
        "        \n",
        "\n",
        "    def call(self,input):\n",
        "        x  = self.dense(input)\n",
        "        phi= self.activation(x)\n",
        "        mean = phi[:,:128]\n",
        "        std =tf.math.exp(phi[:,128:])\n",
        "        epsilon = K.random_normal(shape = K.constant((mean.shape[1], ), dtype = 'int32'))\n",
        "        output = mean + std*epsilon\n",
        "        return output,phi\n",
        "\n",
        "class EmbeddingCompresssor(keras.Model):\n",
        "    def __init__(self):\n",
        "        super(EmbeddingCompresssor, self).__init__()\n",
        "        self.dense = L.Dense(128)\n",
        "\n",
        "    def call(self,input):\n",
        "        x = self.dense(input)\n",
        "        x = L.LeakyReLU(0.2)(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "############################################################\n",
        "# Stage 1 Generator Network (CGAN)\n",
        "############################################################\n",
        "\n",
        "\n",
        "def UpSamplingBlock(input,num_filters):\n",
        "    x = L.UpSampling2D(size=2)(input)\n",
        "    x = L.Conv2D(num_filters,kernel_size=3,padding='same',strides=1,use_bias=False)(x)\n",
        "    x = L.BatchNormalization()(x)\n",
        "    x = L.ReLU()(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class Stage1Generator(keras.Model):\n",
        "    def __init__(self):\n",
        "        super(Stage1Generator, self).__init__(name='stage_1_generator')\n",
        "        self.augmentation = ConditioningAugmentation()\n",
        "        self.concat = L.Concatenate(axis=1)\n",
        "        self.dense = tf.keras.layers.Dense(units = 128*8*4*4, kernel_initializer = tf.random_normal_initializer(stddev = 0.02))\n",
        "        self.reshape = tf.keras.layers.Reshape(target_shape = (4, 4, 128*8), input_shape = (128*8*4*4, ))\n",
        "        self.batchnorm1 = tf.keras.layers.BatchNormalization(axis = -1, momentum = 0.99)\n",
        "        self.activation = L.ReLU()\n",
        "\n",
        "    def call(self,inputs):\n",
        "        embedding , noise = inputs\n",
        "        c , phi = self.augmentation(embedding)\n",
        "        gen_input = self.concat([c,noise])\n",
        "        x = self.dense(gen_input)\n",
        "        x = self.reshape(x)\n",
        "        x = self.batchnorm1(x)\n",
        "        x = self.activation(x)\n",
        "        x = UpSamplingBlock(x, 512)\n",
        "        x = UpSamplingBlock(x, 256)\n",
        "        x = UpSamplingBlock(x,128)\n",
        "        x = UpSamplingBlock(x,3)\n",
        "        x = L.Conv2D(3,kernel_size=3,padding='same')(x)\n",
        "        x = L.Activation('tanh')(x)\n",
        "\n",
        "        return x,phi\n",
        "\n",
        "class Stage1Discriminator(keras.Model):\n",
        "    def __init__(self,*args, **kwargs):\n",
        "        super(Stage1Discriminator, self).__init__(*args,**kwargs)\n",
        "        self.l1 = L.Conv2D(64,kernel_size=4,strides=2,padding='same',kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev = 0.02 ))\n",
        "        self.l2 = L.Conv2D(128,kernel_size=4,strides=2,padding='same',kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev =0.02))\n",
        "        self.l3 = L.BatchNormalization(axis = -1)\n",
        "        self.l4 = L.Conv2D(256,kernel_size=4,strides=2,padding='same',kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n",
        "        self.l5 = L.BatchNormalization(axis = -1)\n",
        "        self.l6 = L.Conv2D(516,kernel_size=4,strides=2,padding='same',kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n",
        "        self.l7 = L.BatchNormalization(axis = -1)\n",
        "        self.embedding = EmbeddingCompresssor()\n",
        "        self.l9 = L.Reshape(target_shape = (1,1,128))\n",
        "        self.concat= L.Concatenate() \n",
        "        self.l11= L.Conv2D(filters = 1024, kernel_size=4,strides=2,padding='same',kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n",
        "        self.l12= L.BatchNormalization(axis = -1)\n",
        "        self.l13= L.Conv2D(filters = 1, kernel_size=4,strides=2,padding='same',kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n",
        "        \n",
        "\n",
        "    def call(self,inputs):\n",
        "        I , E = inputs #\n",
        "        x  = self.l1(I)\n",
        "        x  = self.l2(x)\n",
        "        x  = self.l3(x)\n",
        "        x  = self.l4(x)\n",
        "        x  = self.l5(x)\n",
        "        x  = self.l6(x)\n",
        "        x  = self.l7(x)\n",
        "\n",
        "        t  = self.embedding(E)\n",
        "        t  = self.l9(t)\n",
        "        t  = tf.tile(t,(1,4,4,1))\n",
        "\n",
        "        merged_input  = self.concat([t,x])\n",
        "\n",
        "        y = self.l11(merged_input)\n",
        "        y =self.l12(y)\n",
        "        y =L.LeakyReLU()(y)\n",
        "\n",
        "        y = self.l13(y)\n",
        "        y = tf.squeeze(y)\n",
        "        return y\n",
        "\n",
        "\n",
        "def KL_loss(y_true, y_pred):\n",
        "  mean = y_true[:, :128]\n",
        "  logsigma = y_pred[:, 128:]\n",
        "  loss = -logsigma + 0.5*(-1 + K.exp(2.0*logsigma) + K.square(mean))\n",
        "  loss = K.mean(loss)\n",
        "  return loss\n",
        "\n",
        "class Stage1Model(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(Stage1Model, self).__init__()\n",
        "    self.generator = Stage1Generator()\n",
        "    self.discriminator = Stage1Discriminator()\n",
        "    self.generator_optimizer = keras.optimizers.Adam(learning_rate= 0.001, beta_1= 0.5 , beta_2= 0.999)\n",
        "    self.discriminator_optimizer = keras.optimizers.Adam(learning_rate= 0.001, beta_1= 0.5 , beta_2= 0.999)\n",
        "    self.noise_dim = 100\n",
        "    self.c_dim = 128\n",
        "    self.loss = {}\n",
        "\n",
        "\n",
        "  def load_weights(self,path):\n",
        "    z_noise = tf.random.normal((1, self.noise_dim))\n",
        "    embedding = tf.random.normal((1,1024))\n",
        "    image, phi = self.generator([embedding, z_noise])\n",
        "    logit = self.discriminator([image,embedding])\n",
        "    self.generator.load_weights(path+\"/stage1_generator.h5\")\n",
        "    self.discriminator.load_weights(path+\"/stage1_discriminator.h5\")\n",
        "\n",
        "  def train(self, train_ds, batch_size = 64, num_epochs = 600,save_weights_epoch=5,train_length=8855):\n",
        "    for epoch in range(num_epochs):\n",
        "      print(\"Epoch %d/%d:\\n \"%(epoch + 1, num_epochs), end = \"\")\n",
        "      start_time = time.time()\n",
        "      if epoch % 100 == 0:\n",
        "        K.set_value( self.generator_optimizer.learning_rate,  self.generator_optimizer.learning_rate / 2)\n",
        "        K.set_value( self.generator_optimizer.learning_rate,  self.generator_optimizer.learning_rate / 2)\n",
        "    \n",
        "      generator_loss_log = []\n",
        "      discriminator_loss_log = []\n",
        "      steps_per_epoch = train_length//batch_size\n",
        "      batch_iter = iter(train_ds)\n",
        "      for i in range(steps_per_epoch):\n",
        "        if i % 5 == 0:\n",
        "          print(\"=\", end = \"\")\n",
        "        image_batch, embedding_batch = next(batch_iter)\n",
        "        batch_size = image_batch.shape[0]\n",
        "        z_noise = tf.random.normal((batch_size, self.noise_dim))\n",
        "\n",
        "        mismatched_images = tf.roll(image_batch, shift = 1, axis = 0)\n",
        "\n",
        "        real_labels = tf.random.uniform(shape = (batch_size, ), minval = 0.9, maxval = 1.0)\n",
        "        fake_labels = tf.random.uniform(shape = (batch_size, ), minval = 0.0, maxval = 0.1)\n",
        "        mismatched_labels = tf.random.uniform(shape = (batch_size, ), minval = 0.0, maxval = 0.1)\n",
        "\n",
        "        with tf.GradientTape() as generator_tape, tf.GradientTape() as discriminator_tape:\n",
        "          fake_images, phi = self.generator([embedding_batch, z_noise])\n",
        "          \n",
        "          real_logits = self.discriminator([image_batch, embedding_batch])\n",
        "          fake_logits = self.discriminator([fake_images, embedding_batch])\n",
        "          mismatched_logits = self.discriminator([mismatched_images, embedding_batch])\n",
        "\n",
        "          l_sup = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(real_labels, fake_logits))\n",
        "          #l_klreg = KL_loss(tf.random.normal((phi.shape[0], phi.shape[1])), phi)\n",
        "          generator_loss = l_sup \n",
        "          #generator_loss = l_sup + 2.0*l_klreg\n",
        "          \n",
        "          l_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(real_labels, real_logits))\n",
        "          l_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(fake_labels, fake_logits))\n",
        "          l_mismatched = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(mismatched_labels, mismatched_logits))\n",
        "          discriminator_loss = 0.5*tf.add(l_real, 0.5*tf.add(l_fake, l_mismatched))\n",
        "        \n",
        "        generator_gradients = generator_tape.gradient(generator_loss, self.generator.trainable_variables)\n",
        "        discriminator_gradients = discriminator_tape.gradient(discriminator_loss, self.discriminator.trainable_variables)\n",
        "        \n",
        "        self.generator_optimizer.apply_gradients(zip(generator_gradients, self.generator.trainable_variables))\n",
        "        self.discriminator_optimizer.apply_gradients(zip(discriminator_gradients, self.discriminator.trainable_variables))\n",
        "        \n",
        "        generator_loss_log.append(generator_loss)\n",
        "        discriminator_loss_log.append(discriminator_loss)\n",
        "\n",
        "      end_time = time.time()\n",
        "\n",
        "      if epoch % 1 == 0:\n",
        "        epoch_time = end_time - start_time\n",
        "        template = \" - generator_loss: {:.4f} - discriminator_loss: {:.4f} - epoch_time: {:.2f} s\"\n",
        "        print(template.format(tf.reduce_mean(generator_loss_log), tf.reduce_mean(discriminator_loss_log), epoch_time))\n",
        "\n",
        "      if (epoch + 1) % save_weights_epoch == 0 or epoch == num_epochs - 1:\n",
        "        save_path = \"./lr_results/epoch_\" + str(epoch + 1)\n",
        "        temp_embeddings = None\n",
        "        for _, embeddings in train_ds:\n",
        "          temp_embeddings = embeddings.numpy()\n",
        "          break\n",
        "        if os.path.exists(save_path) == False:\n",
        "          os.makedirs(save_path)\n",
        "        temp_batch_size = 10\n",
        "        temp_z_noise = tf.random.normal((temp_batch_size, self.noise_dim))\n",
        "        temp_embedding_batch = temp_embeddings[0:temp_batch_size]\n",
        "        fake_images, _ = self.generator([temp_embedding_batch, temp_z_noise])\n",
        "        for i, image in enumerate(fake_images):\n",
        "          image = 127.5*image + 127.5\n",
        "          image = image.numpy().astype('uint8')\n",
        "          image  = keras.preprocessing.image.array_to_img(image)\n",
        "          image.save(save_path + \"/gen_%d.png\"%(i))\n",
        "\n",
        "        weights_path = f\"./weights/weights_{epoch+1}\"\n",
        "\n",
        "        if os.path.exists(weights_path)== False:\n",
        "          os.makedirs(weights_path)\n",
        "        self.generator.save_weights(weights_path+\"/stage1_generator.h5\")\n",
        "        self.discriminator.save_weights(weights_path+\"/stage1_discriminator.h5\")\n",
        "\n",
        "    \n",
        "def generate_image(self, embedding, batch_size= 64):\n",
        "    self.generator.compile(loss = \"mse\", optimizer = \"adam\")\n",
        "    self.generator.load_weights(\"stage1_generator_600.ckpt\").expect_partial()\n",
        "    z_noise = tf.random.normal((batch_size, self.noise_dim))\n",
        "    generated_image = self.generator([embedding, z_noise])\n",
        "    return generated_image                \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6R9D3W-JO5m"
      },
      "source": [
        "with strategy.scope():\n",
        "  model = Stage1Model()\n",
        "  model.load_weights(path=\"./weights/weights_50\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOvk5WW7xxIT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "59a3f11d-bc90-4e2c-afce-cf3c5e7be510"
      },
      "source": [
        "model.train(train_ds,batch_size=32,num_epochs=20,save_weights_epoch=5)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20:\n",
            " ="
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "UnavailableError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnavailableError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-b7c7d07751cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msave_weights_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-98ee86adb511>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_ds, batch_size, num_epochs, save_weights_epoch, train_length)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0mimage_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mz_noise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    759\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    751\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 753\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mexecution_mode\u001b[0;34m(mode)\u001b[0m\n\u001b[1;32m   2193\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2194\u001b[0m       \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor_old\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2195\u001b[0;31m       \u001b[0mexecutor_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/executor.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;34m\"\"\"Waits for ops dispatched in this executor to finish.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mpywrap_tfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFE_ExecutorWaitForAllPendingNodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mclear_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnavailableError\u001b[0m: failed to connect to all addresses\nAdditional GRPC error information from remote target /job:localhost/replica:0/task:0/device:CPU:0:\n:{\"created\":\"@1627486890.584616018\",\"description\":\"Failed to pick subchannel\",\"file\":\"third_party/grpc/src/core/ext/filters/client_channel/client_channel.cc\",\"file_line\":5420,\"referenced_errors\":[{\"created\":\"@1627486890.584614354\",\"description\":\"failed to connect to all addresses\",\"file\":\"third_party/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc\",\"file_line\":398,\"grpc_status\":14}]}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XE5CGdWAO1oc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0e7c519-df68-4a29-ae9b-341f605fa2b3"
      },
      "source": [
        "model.generator.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"stage_1_generator\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conditioning_augmentation_5  multiple                  262400    \n",
            "_________________________________________________________________\n",
            "concatenate_10 (Concatenate) multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             multiple                  3751936   \n",
            "_________________________________________________________________\n",
            "reshape_10 (Reshape)         multiple                  0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_51455 (B multiple                  4096      \n",
            "_________________________________________________________________\n",
            "re_lu_51435 (ReLU)           multiple                  0         \n",
            "=================================================================\n",
            "Total params: 4,018,432\n",
            "Trainable params: 4,016,384\n",
            "Non-trainable params: 2,048\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pI9uhvhlWFoZ"
      },
      "source": [
        "\n",
        "dataset = Dataset(image_size=(256,256),data_base_path=\"./data\",batch_size=1)\n",
        "train_ds = dataset.get_test_ds()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neAbJUlJNiV9"
      },
      "source": [
        "len(dataset.test_filenames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0I3i7kPFKM8I"
      },
      "source": [
        "def ResidualBlock(input, num_filters):\n",
        "    x  =  L.Conv2D(filters = num_filters, kernel_size= 3 , strides=1, padding='same')(input)\n",
        "    x  =  L.BatchNormalization()(x)\n",
        "    x  =  L.ReLU()(x)\n",
        "    x  =  L.Conv2D(filters = num_filters, kernel_size=3, strides=1, padding='same')(x)\n",
        "    x  =  L.BatchNormalization()(x)\n",
        "    x  =  L.ReLU()(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class Stage2Generator(keras.Model):\n",
        "    def __init__(self,*args, **kwargs):\n",
        "        super(Stage2Generator, self).__init__(*args, **kwargs)\n",
        "\n",
        "    def call(self,inputs):\n",
        "        x = ResidualBlock(inputs, 128)\n",
        "        x = ResidualBlock(x,256)\n",
        "        x = UpSamplingBlock(x,256)\n",
        "        x = ResidualBlock(x,128)\n",
        "        x = UpSamplingBlock(x,256)\n",
        "        x = ResidualBlock(x,3)\n",
        "        return x\n",
        "\n",
        "def DownSamplingBlock(  inputs,\n",
        "                        num_filters, \n",
        "                        kernel_size= 4,\n",
        "                        strides = 2,\n",
        "                        batch_norm=True,\n",
        "                        activation= True):\n",
        "    x = L.Conv2D(filters = num_filters, kernel_size= kernel_size , strides=strides, padding='same')(inputs)\n",
        "    if batch_norm:\n",
        "        x = L.BatchNormalization()(x)\n",
        "    if activation:\n",
        "        x = L.LeakyReLU()(x)\n",
        "    #print(f\" x shape {x.shape}\")\n",
        "    return x\n",
        "\n",
        "\n",
        "class Stage2Discriminator(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(Stage2Discriminator, self).__init__()\n",
        "    self.embed = EmbeddingCompresssor()\n",
        "    self.reshape = tf.keras.layers.Reshape(target_shape = (1, 1, 128))\n",
        "    self.conv_out = tf.keras.layers.Conv2D(filters = 1, kernel_size = 4, strides = 1, padding = \"valid\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n",
        "\n",
        "  def call(self, inputs):\n",
        "    I, E = inputs\n",
        "    T = self.embed(E)\n",
        "    T = self.reshape(T)\n",
        "    T = tf.tile(T, (1, 4, 4, 1))\n",
        "    \n",
        "    x = DownSamplingBlock(I,num_filters=64,batch_norm=False)\n",
        "    x = DownSamplingBlock(x,num_filters=128)\n",
        "    x = DownSamplingBlock(x, num_filters=256)\n",
        "    x = DownSamplingBlock(x,num_filters=512)\n",
        "    x = DownSamplingBlock(x,num_filters=1024)\n",
        "    x = DownSamplingBlock(x,num_filters=512)\n",
        "    x = DownSamplingBlock(x,num_filters=128,kernel_size=1,strides=1)\n",
        "    \n",
        "    y = DownSamplingBlock(x, num_filters=128,kernel_size=1,strides=1)\n",
        "    y = DownSamplingBlock(y, num_filters=256,kernel_size=3,strides=1)\n",
        "    y = DownSamplingBlock(y, num_filters=128,kernel_size=3,strides=1)\n",
        "\n",
        "    A = tf.keras.layers.Add()([x,y])\n",
        "    A = tf.nn.leaky_relu(A)\n",
        "    merged_input = tf.keras.layers.concatenate([A, T])\n",
        "    \n",
        "    z = DownSamplingBlock(merged_input,128,kernel_size=1,strides=1)\n",
        "    z = self.conv_out(z)\n",
        "    print(f\" z {z.shape}\")\n",
        "    z = \n",
        "    print(f\" z {z.shape} and z :{z}\")\n",
        "\n",
        "    return z\n",
        "\n",
        "\n",
        "class Stage2Model(keras.Model):\n",
        "    def __init__(self):\n",
        "        super(Stage2Model, self).__init__()\n",
        "        self.generator1 = Stage1Generator()\n",
        "        self.generator2 = Stage2Generator()\n",
        "        self.discriminator2 = Stage2Discriminator()\n",
        "        self.generator2_optimizer = keras.optimizers.Adam(learning_rate= 0.0001, beta_1= 0.5 , beta_2= 0.999)\n",
        "        self.discriminator2_optimizer = keras.optimizers.Adam(learning_rate= 0.0001, beta_1= 0.5 , beta_2= 0.999)\n",
        "        self.noise_dim = 100\n",
        "    def train(self, train_ds, batch_size= 64, num_epochs =1,steps_per_epoch =125):\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            print(\"Epoch %d/%d:\\n  \"%(epoch + 1, num_epochs), end = \"\")\n",
        "            start_time = time.time()\n",
        "            if epoch % 100 == 0:\n",
        "                K.set_value(self.generator2_optimizer.learning_rate, self.generator2_optimizer.learning_rate / 2)\n",
        "                K.set_value(self.discriminator2_optimizer.learning_rate, self.discriminator2_optimizer.learning_rate / 2)\n",
        "            \n",
        "            generator_loss_log = []\n",
        "            discriminator_loss_log = []\n",
        "            steps_per_epoch = steps_per_epoch\n",
        "            batch_iter = iter(train_ds) \n",
        "            for i in range(steps_per_epoch):\n",
        "                if i% 5 ==0:\n",
        "                    print(\"=\", end = \"\")\n",
        "\n",
        "                hr_image_batch,embedding_batch = next(batch_iter)\n",
        "                batch_size = hr_image_batch.shape[0]\n",
        "                z_noise = tf.random.normal((batch_size, self.noise_dim))\n",
        "                mismatched_images = tf.roll(hr_image_batch, shift=1, axis = 0)\n",
        "\n",
        "                real_labels = tf.random.uniform(shape = (batch_size, ), minval = 0.9, maxval=1.0)\n",
        "                fake_labels = tf.random.uniform(shape = (batch_size, ), minval = 0.0 , maxval = 0.1)\n",
        "                mismatched_labels = tf.random.uniform(shape = (batch_size, ), minval = 0.0, maxval = 0.1)\n",
        "\n",
        "                with tf.GradientTape() as generator_tape, tf.GradientTape() as discriminator_tape:\n",
        "                    lr_fake_images , phi = self.generator1([embedding_batch,z_noise])\n",
        "                    hr_fake_images        = self.generator2(lr_fake_images)\n",
        "                    real_logits = self.discriminator2([hr_image_batch, embedding_batch])\n",
        "                    fake_logits = self.discriminator2([hr_fake_images, embedding_batch])\n",
        "                    mismatched_logits = self.discriminator2([mismatched_images, embedding_batch])\n",
        "\n",
        "                    l_sup = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(real_labels,real_logits))\n",
        "                    l_klreg = KL_loss(tf.random.normal((phi.shape[0], phi.shape[1])), phi)\n",
        "                    generator_loss = l_sup + 2.0*l_klreg\n",
        "                    \n",
        "                    \n",
        "                    l_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(real_labels,real_logits))\n",
        "                    l_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(fake_logits,fake_labels))\n",
        "                    l_mismatched = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(mismatched_logits,mismatched_labels))\n",
        "                    discriminator_loss = 0.5*tf.add(l_real,tf.add(l_fake, l_mismatched))\n",
        "\n",
        "                generator_gradients = generator_tape.gradient(generator_loss,self.generator2.trainable_variables)\n",
        "                self.generator2_optimizer.apply_gradients(zip(generator_gradients, self.generator2.trainable_variables))             \n",
        "\n",
        "                discriminator_gradients = discriminator_tape.gradient(discriminator_loss,self.discriminator2.trainable_variables)\n",
        "                self.discriminator2_optimizer.apply_gradients(zip(discriminator_gradients, self.discriminator2.trainable_variables))\n",
        "\n",
        "                generator_loss_log.append(generator_loss)\n",
        "                discriminator_loss_log.append(discriminator_loss)\n",
        "\n",
        "                end_time = time.time()\n",
        "\n",
        "                if epoch % 1 == 0:\n",
        "                    epoch_time = end_time - start_time\n",
        "                    template = \" - generator_loss: {:.4f} - discriminator_loss: {:.4f} - epoch_time: {:.2f} s\"\n",
        "                    print(template.format(tf.reduce_mean(generator_loss_log), tf.reduce_mean(discriminator_loss_log), epoch_time))\n",
        "\n",
        "                if (epoch + 1) % 10 == 0 or epoch == num_epochs - 1:\n",
        "                    save_path = \"./hr_results/epoch_\" + str(epoch + 1)\n",
        "                    temp_embeddings = None\n",
        "                    for _, embeddings in train_ds:\n",
        "                        temp_embeddings = embeddings.numpy()\n",
        "                        break\n",
        "                    if os.path.exists(save_path) == False:\n",
        "                        os.makedirs(save_path)\n",
        "                    temp_batch_size = 10\n",
        "                    temp_z_noise = tf.random.normal((temp_batch_size, self.noise_dim))\n",
        "                    temp_embedding_batch = temp_embeddings[0:temp_batch_size]\n",
        "                    fake_images, _ = self.generator2([temp_embedding_batch, temp_z_noise])\n",
        "                    for i, image in enumerate(fake_images):\n",
        "                            image = 127.5*image + 127.5\n",
        "                            image = image.numpy().astype('uint8')\n",
        "                            image  = keras.preprocessing.image.array_to_img(image)\n",
        "                            image.save(save_path + \"/gen_%d.png\"%(i))\n",
        "\n",
        "                    weights_path = f\"./weights/hr_weights_{epoch+1}\"\n",
        "                    if os.path.exists(weights_path)== False:\n",
        "                        os.mkdirs(weights_path)\n",
        "                    self.generator2.save_weights(weights_path+\"/stage2_generator.h5\")\n",
        "                    self.discriminator2.save_weights(weights_path+\"/stage2_discriminator.h5\")\n",
        "\n",
        "\n",
        "model2= Stage2Model()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMKBd1CxWsuc"
      },
      "source": [
        "model2.train(train_ds,batch_size=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WEh-oXVOqiJ"
      },
      "source": [
        "  "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}